# Mutual Information Neural Estimation

This repository provides codes reproducing a number of experimental results in [Mutual Information Neural Estimation](https://arxiv.org/abs/1801.04062).

## Methodology 

### Background


### Using the Donsker-Varadhan dual representation of KL-divergence


### Implementation

**_mine.py_** provides the implementation of the mutual information estimated with the statistical network. We use a two-layer multi-layer-perceptrons with a sigmoid activation for the statistical network.

## Results

**Exp1: True MI vs Neural estimated MI**


**Exp2: Capturing non-linear dependencies** 

* Visualization of the examples generated by the non-linear function
* Results

**Exp3: Maximizing mutual information to improve GANs**
